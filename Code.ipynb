{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZERU6zXcViq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/gd'):\n",
        "    drive.mount('/gd')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu -q\n"
      ],
      "metadata": {
        "id": "BWNIQZBFcbw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def gpe(x=None):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = gpe"
      ],
      "metadata": {
        "id": "mR9EEbNucdxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zkI8D8gichcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trans_df = pd.read_excel('/content/sau_std.xlsx')\n",
        "print(trans_df.shape)\n",
        "print(trans_df.columns)"
      ],
      "metadata": {
        "id": "NxK6Um-ccnGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 100"
      ],
      "metadata": {
        "id": "Y8Jj76TicoFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df.sample(10)"
      ],
      "metadata": {
        "id": "NVMGq8u1croR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df.isnull().sum()"
      ],
      "metadata": {
        "id": "AR5OfSgvc2Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df.split.value_counts()"
      ],
      "metadata": {
        "id": "p1btZQNYc3EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = trans_df[trans_df.split=='train'].copy()\n",
        "df_dev = trans_df[trans_df.split=='dev'].copy()\n",
        "df_test = trans_df[trans_df.split=='test'].copy()"
      ],
      "metadata": {
        "id": "WIFH4Tqxc39D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import NllbTokenizer\n",
        "from tqdm.auto import tqdm, trange"
      ],
      "metadata": {
        "id": "yQgSnx1jc8AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NllbTokenizer.from_pretrained('facebook/m2m100_418M')"
      ],
      "metadata": {
        "id": "swMZA0jyc87V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def word_tokenize(text):\n",
        "    return re.findall('(\\w+|[^\\w\\s])', text)"
      ],
      "metadata": {
        "id": "QAHyfD8VdDCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "smpl = df_train.sample(400, random_state=1)\n",
        "smpl['std_toks'] = smpl['std'].apply(tokenize_text)\n",
        "smpl['sau_toks'] = smpl['sau'].apply(tokenize_text)\n",
        "\n",
        "smpl['std_words'] = smpl['std'].apply(word_tokenize)\n",
        "smpl['sau_words'] = smpl['sau'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "nCknhv-LdEIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smpl.sample(5)[['sau', 'sau_words', 'sau_toks', 'std', 'std_words', 'std_toks']]"
      ],
      "metadata": {
        "id": "SR8cXJdJdFc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = smpl[['std_toks', 'sau_toks', 'std_words', 'sau_words']].applymap(len).describe()\n",
        "stats"
      ],
      "metadata": {
        "id": "uZbjQIindJh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stats.std_toks['mean'] / stats.std_words['mean'])\n",
        "print(stats.sau_toks['mean'] / stats.sau_words['mean'])"
      ],
      "metadata": {
        "id": "DAHHlNqpdJzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.unk_token, tokenizer.unk_token_id)"
      ],
      "metadata": {
        "id": "vrhAcFU7dKTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_unk = [text for text in tqdm(trans_df.sau) if tokenizer.unk_token_id in tokenizer(text).input_ids]\n",
        "print(len(texts_with_unk))"
      ],
      "metadata": {
        "id": "Lcz23jubdcQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "s = random.sample(texts_with_unk, min(len(texts_with_unk), 5))\n",
        "s"
      ],
      "metadata": {
        "id": "JpTYcg2tddZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import typing as tp\n",
        "import unicodedata\n",
        "from sacremoses import MosesPunctNormalizer\n",
        "\n",
        "\n",
        "mpn = MosesPunctNormalizer(lang=\"en\")\n",
        "mpn.substitutions = [\n",
        "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
        "]\n",
        "\n",
        "\n",
        "def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n",
        "    non_printable_map = {\n",
        "        ord(c): replace_by\n",
        "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
        "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
        "    }\n",
        "\n",
        "    def replace_non_printing_char(line) -> str:\n",
        "        return line.translate(non_printable_map)\n",
        "\n",
        "    return replace_non_printing_char\n",
        "\n",
        "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
        "\n",
        "def preproc(text):\n",
        "    clean = mpn.normalize(text)\n",
        "    clean = replace_nonprint(clean)\n",
        "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
        "    return clean"
      ],
      "metadata": {
        "id": "LkTuHmKpdeQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_unk_normed = [text for text in tqdm(texts_with_unk) if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids]\n",
        "print(len(texts_with_unk_normed))"
      ],
      "metadata": {
        "id": "uEYIJtoAdkcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import NllbTokenizer"
      ],
      "metadata": {
        "id": "xToWq3PedkTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "8U57Iq-sdjyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n",
        "print(len(tokenizer))\n",
        "print(tokenizer.convert_ids_to_tokens([256202, 256203]))"
      ],
      "metadata": {
        "id": "2c_dtK6edvHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_tokenizer(tokenizer, new_lang='sau_Cyrl'):\n",
        "\n",
        "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
        "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
        "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
        "\n",
        "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
        "\n",
        "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
        "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
        "    if new_lang not in tokenizer._additional_special_tokens:\n",
        "        tokenizer._additional_special_tokens.append(new_lang)\n",
        "\n",
        "    tokenizer.added_tokens_encoder = {}\n",
        "    tokenizer.added_tokens_decoder = {}"
      ],
      "metadata": {
        "id": "H-qe6vebdvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens([256202, 256203, 256204]))\n",
        "print(tokenizer.convert_tokens_to_ids(['zul_Latn', 'tyv_Cyrl', '<mask>']))\n"
      ],
      "metadata": {
        "id": "eIehUVF6dvBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "added_token_id = tokenizer.convert_tokens_to_ids('sau_Cyrl')\n",
        "similar_lang_id = tokenizer.convert_tokens_to_ids('kir_Cyrl')\n",
        "print(added_token_id, similar_lang_id)"
      ],
      "metadata": {
        "id": "RBfTAt78du-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "KYEQBw3jd88v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.model.shared.weight.data[added_token_id+1] = model.model.shared.weight.data[added_token_id]\n",
        "model.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]"
      ],
      "metadata": {
        "id": "XoXc85HVd85K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm, trange\n",
        "from transformers.optimization import Adafactor\n",
        "from transformers import get_constant_schedule_with_warmup\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Try to free GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "5FWYBBmid82r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda();"
      ],
      "metadata": {
        "id": "jbuOfJQLeNlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adafactor(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    scale_parameter=False,\n",
        "    relative_step=False,\n",
        "    lr=1e-4,\n",
        "    clip_threshold=1.0,\n",
        "    weight_decay=1e-3,\n",
        ")"
      ],
      "metadata": {
        "id": "PgvDwChgeNdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "max_length = 32\n",
        "warmup_steps = 1_000\n",
        "training_steps = 5700"
      ],
      "metadata": {
        "id": "BKfzqRcueVlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)"
      ],
      "metadata": {
        "id": "QUnR2dTjeViP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGS = [('std', 'guj_Gujr'), ('sau', 'sau_Cyrl')]\n",
        "\n",
        "def get_batch_pairs(batch_size, data=df_train):\n",
        "    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n",
        "    xx, yy = [], []\n",
        "    for _ in range(batch_size):\n",
        "        item = data.iloc[random.randint(0, len(data)-1)]\n",
        "        xx.append(preproc(item[l1]))\n",
        "        yy.append(preproc(item[l2]))\n",
        "    return xx, yy, long1, long2\n",
        "\n",
        "print(get_batch_pairs(1))\n"
      ],
      "metadata": {
        "id": "MVuDd09VeVfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_PATH = '/gd/MyDrive/models/nllb-sau-guj-v1'"
      ],
      "metadata": {
        "id": "1cfbNN4teVdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "x, y, loss = None, None, None\n",
        "cleanup()\n",
        "\n",
        "tq = trange(len(losses), training_steps)\n",
        "for i in tq:\n",
        "    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n",
        "    try:\n",
        "        tokenizer.src_lang = lang1\n",
        "        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "        tokenizer.src_lang = lang2\n",
        "        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        loss = model(**x, labels=y.input_ids).loss\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scheduler.step()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        x, y, loss = None, None, None\n",
        "        cleanup()\n",
        "        print('error', max(len(s) for s in xx + yy), e)\n",
        "        continue\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(i, np.mean(losses[-1000:]))\n",
        "\n",
        "    if i % 1000 == 0 and i > 0:\n",
        "        model.save_pretrained(MODEL_SAVE_PATH)\n",
        "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "cYIpMuhxee2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(losses).ewm(100).mean().plot();"
      ],
      "metadata": {
        "id": "-Z0AhjEreeya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "G9aAyg9pejZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_load_name = '/gd/MyDrive/models/nllb-sau-guj-v1'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\n",
        "tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n",
        "fix_tokenizer(tokenizer)"
      ],
      "metadata": {
        "id": "ngpvLrajet9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(\n",
        "    text, src_lang='guj_Gujr', tgt_lang='eng_Latn',\n",
        "    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n",
        "):\n",
        "    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n",
        "    tokenizer.src_lang = src_lang\n",
        "    tokenizer.tgt_lang = tgt_lang\n",
        "    inputs = tokenizer(\n",
        "        text, return_tensors='pt', padding=True, truncation=True,\n",
        "        max_length=max_input_length\n",
        "    )\n",
        "    model.eval() # turn off training mode\n",
        "    result = model.generate(\n",
        "        **inputs.to(model.device),\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        num_beams=num_beams, **kwargs\n",
        "    )\n",
        "    return tokenizer.batch_decode(result, skip_special_tokens=True)\n",
        "\n",
        "# Example usage:\n",
        "t = \"ખાડા માં ખાબકીને કેમ તારો ખીસ્સો ખાલી થઇ ગયો  \"\n",
        "print(translate(t, 'sau_Cyrl', 'guj_Gujr'))\n"
      ],
      "metadata": {
        "id": "4YSz8yyIet6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}